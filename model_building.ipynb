{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Tuple\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_parse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaParse\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_parse'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio  \n",
    "nest_asyncio.apply()\n",
    "\n",
    "from typing import List, Tuple\n",
    "from llama_parse import LlamaParse\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=\"llx-3QORP75OUx11inHUpIy67FLzIgYc0gjfAGKRLDiECXOXkkne\",\n",
    "    result_type=\"markdown\",\n",
    "    num_workers=1,\n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    ")\n",
    "\n",
    "def load_text_files(folder_path: str) -> str:\n",
    "    all_text = \"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                all_text += f.read() + \"\\n\"\n",
    "    return all_text\n",
    "\n",
    "def create_vector_database(text_data: str, data_dir=\"faiss_data\") -> Tuple:\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=256\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text_data)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=documents, \n",
    "        embedding=embed_model\n",
    "    )\n",
    "\n",
    "    faiss_index_path = os.path.join(data_dir, \"faiss_index\")\n",
    "    vector_store.save_local(faiss_index_path)\n",
    "\n",
    "    return vector_store, embed_model\n",
    "text_data = load_text_files(DATA_FOLDER)\n",
    "\n",
    "vector_db, embed_model = create_vector_database(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_15304\\297736213.py:31: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
      "c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def dataframes_to_text(document) -> str:\n",
    "    all_text = \"\"\n",
    "    for doc in document:\n",
    "        if hasattr(doc, 'text_resource') and doc.text_resource:\n",
    "            all_text += doc.text_resource.text + \"\\n\"\n",
    "    return all_text\n",
    "\n",
    "def create_vector_database(document, data_dir=\"faiss_data\") -> Tuple:\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    all_text = dataframes_to_text(document)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=256,\n",
    "        chunk_overlap=64\n",
    "    )\n",
    "    chunks = text_splitter.split_text(all_text)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=documents, \n",
    "        embedding=embed_model\n",
    "    )\n",
    "\n",
    "    faiss_index_path = os.path.join(data_dir, \"faiss_index\")\n",
    "    vector_store.save_local(faiss_index_path)\n",
    "    \n",
    "    return vector_store, embed_model\n",
    "vector_db, embed_model = create_vector_database(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello! I'm your Nano Finance Regulation assistant. Ask me anything!\n",
      "(Type 'quit' to exit)\n",
      "Bot: สรุปเกี่ยวกับ Data Management Concerns จากข้อมูลที่มีอยู่:\n",
      "\n",
      "1. **การวางแผน**: การวางแผนที่ดีสำหรับการจัดการข้อมูลเป็นสิ่งสำคัญ เพื่อให้แน่ใจว่าข้อมูลถูกจัดเก็บและเข้าถึงได้อย่างเหมาะสม\n",
      "2. **การควบคุม**: การควบคุมข้อมูลเพื่อป้องกันการสูญหาย การเข้าถึงที่ไม่ได้รับอนุญาต หรือการใช้งานข้อมูลในทางที่ผิด\n",
      "3. **การจัดส่ง**: การจัดส่งข้อมูลให้กับผู้ใช้ที่ต้องการอย่างมีประสิทธิภาพ\n",
      "4. **คุณภาพข้อมูล**: การรักษาคุณภาพของข้อมูลให้ดี เพื่อให้ข้อมูลที่ถูกต้องและเชื่อถือได้\n",
      "5. **ความปลอดภัยข้อมูล**: การปกป้องข้อมูลจากการเข้าถึงที่ไม่ได้รับอนุญาตหรือการโจมตี\n",
      "6. **เมตาดาต้า**: การจัดการเมตาดาต้าเพื่อให้สามารถเข้าใจและใช้ข้อมูลได้อย่างมีประสิทธิภาพ\n",
      "7. **ข้อมูลอ้างอิง**: การจัดการข้อมูลอ้างอิงเพื่อให้มั่นใจว่าข้อมูลที่ใช้เป็นมาตรฐานเดียวกัน\n",
      "8. **แพลตฟอร์มการกำกับดูแลข้อมูล**: การใช้แพลตฟอร์มที่เหมาะสมในการกำกับดูแลข้อมูล\n",
      "9. **นโยบาย**: การพัฒนานโยบายที่ชัดเจนสำหรับการจัดการข้อมูล\n",
      "10. **กระบวนการ**: การกำหนดกระบวนการที่ชัดเจนสำหรับการจัดการข้อมูล\n",
      "11. **มาตรฐาน**: การกำหนดมาตรฐานสำหรับการจัดการข้อมูลเพื่อให้สามารถทำงานร่วมกันได้อย่างมีประสิทธิภาพ\n",
      "\n",
      "ข้อมูลเหล่านี้เป็นเพียงส่วนหนึ่งของความกังวลเกี่ยวกับการจัดการข้อมูล และอาจมีปัจจัยอื่นๆ ที่เกี่ยวข้อง แต่จากบริบทที่ให้มา ข้อมูลข้างต้นเป็นภาพรวม\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"sk-GqA4Uj6iZXaykbOzIlFGtmdJr6VqiX94NhhjPZaf81kylRzh\",\n",
    "    openai_api_base=\"https://api.opentyphoon.ai/v1\",\n",
    "    model_name=\"typhoon-v2-70b-instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "def summarize_text(text: str, max_tokens: int = 512) -> str:\n",
    "    tokens = text.split()\n",
    "    return ' '.join(tokens[:max_tokens]) + '...' if len(tokens) > max_tokens else text\n",
    "\n",
    "def generate_response(prompt: str) -> str:\n",
    "    chat_completion = llm.client.chat.completions.create(\n",
    "        model=\"typhoon-v2-70b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant for the Bank of Thailand's Nano Finance Regulation.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "        temperature=0.7,\n",
    "        stop=None\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "def create_chatbot(vector_db):\n",
    "    retriever = vector_db.as_retriever(search_kwargs={'k': 3})\n",
    "    template = \"\"\"\n",
    "    You are an expert assistant tasked with providing detailed, accurate,\n",
    "    and well-contextualized answers based solely on the provided context.\n",
    "    Use the following context from the input documents or vector database to answer the question. \n",
    "    Do not rely on external knowledge unless explicitly stated otherwise. \n",
    "    Ensure your response is comprehensive, directly addresses the question, \n",
    "    and includes relevant details or explanations derived from the context. \n",
    "    If the context does not contain sufficient information to fully answer the question, \n",
    "    clearly state what is missing and provide the best possible response based on what is available.\n",
    "\n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    PROMPT = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm, \n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "def ask_question(qa_chain: RetrievalQA, question: str) -> str:\n",
    "    response = qa_chain({\"query\": question})\n",
    "    return response['result']\n",
    "\n",
    "qa_chain = create_chatbot(vector_db)\n",
    "\n",
    "def chat_loop():\n",
    "    print(\"Bot: Hello! I'm your Nano Finance Regulation assistant. Ask me anything!\")\n",
    "    print(\"(Type 'quit' to exit)\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        response = ask_question(qa_chain, user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
